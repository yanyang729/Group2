{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load helper.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    if isinstance(text, unicode):\n",
    "        text = strip_accents(text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) \\\n",
    "            if contraction_mapping.get(match) \\\n",
    "            else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "from pattern.en import tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    tagged_text = tag(text)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text\n",
    "\n",
    "\n",
    "# lemmatize text based on POS tags\n",
    "def lemmatize_text(text):\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word\n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # remove numbers\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    for text in tqdm(corpus):\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        text = lemmatize_text(text)\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "        else:\n",
    "            normalized_corpus.append(text)\n",
    "    return normalized_corpus\n",
    "\n",
    "\n",
    "def get_topn_tags_transform(path,topn,tag):\n",
    "    \"\"\"\n",
    "    read cleaned data and transform them into one tag per row\n",
    "    1. get top n tags\n",
    "    2. expand row\n",
    "    3. to boolean\n",
    "    4. aggregate by content\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path,quotechar='|',sep=',',header=None)\n",
    "    df.columns = ['title','body','tags']\n",
    "    merged = [ title + ' ' + body for title, body in zip(df.title,df.body)]\n",
    "    df_merged = pd.DataFrame({'content':merged,'tags':df.tags.copy()})\n",
    "    df_merged.tags = df_merged.tags.apply(lambda x: x.replace('<','').split('>')[:-1])\n",
    "    df_transformed = pd.DataFrame(df_merged.tags.tolist(),index=df_merged.content).stack().reset_index()[['content',0]]\n",
    "    df_transformed.columns = ['content','tags']\n",
    "    top_tags = Counter(df_transformed.tags).most_common()[:topn]\n",
    "    top_n_tags = [tag for tag, num in top_tags]\n",
    "    df_filtered = df_transformed[df_transformed.tags.apply(lambda x: x in set(top_n_tags))]\n",
    "    df_filtered.tags = [int(bool) for bool in df_filtered.tags == tag]\n",
    "    df_filtered.columns = ['content','is_{}'.format(tag)]\n",
    "    rslt = df_filtered.groupby('content')['is_{}'.format(tag)].agg(['sum']).reset_index()\n",
    "    rslt.columns = ['content','is_{}'.format(tag)]\n",
    "    return rslt, top_n_tags\n",
    "\n",
    "\n",
    "\n",
    "class ExtractAverageWordVectors(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,g_model):\n",
    "        self.g_model = g_model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.g_model:\n",
    "            return self.averaged_word_vectors(X)\n",
    "\n",
    "    def averaged_word_vectors(self, tokenized_list):\n",
    "        weighted_ave = []\n",
    "\n",
    "        for sentence in tqdm(tokenized_list):\n",
    "            word_vecs = np.array([self.g_model[word] for word in sentence if word in self.g_model])\n",
    "\n",
    "            weighted_ave.append(np.sum(word_vecs, axis = 0)/len(word_vecs))\n",
    "\n",
    "        return np.array(weighted_ave)\n",
    "\n",
    "\n",
    "class ExtractTfidfAveVec(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,corpus, g_model):\n",
    "        self.corpus = corpus\n",
    "        self.g_model = g_model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.ave_weighted_tfidf(X) \n",
    "\n",
    "    def tfidf_extractor(self, ngram_range=(1,1)):\n",
    "\n",
    "        tfidf_obj = TfidfVectorizer(min_df=3, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', ngram_range=ngram_range, use_idf = 1, smooth_idf = 1, sublinear_tf = 1, stop_words='english')\n",
    "\n",
    "        tfidf_features = tfidf_obj.fit_transform(self.corpus)\n",
    "        \n",
    "        return tfidf_obj, tfidf_features\n",
    "\n",
    "    def tfidf_mapper(self, tfidf_obj, tfidf_features):\n",
    "        vocab = tfidf_obj.vocabulary_\n",
    "        words = vocab.keys()\n",
    "        word_tfidfs = [tfidf_features[0, vocab.get(word)] if vocab.get(word) else 0 for word in words]\n",
    "        word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n",
    "        \n",
    "        return word_tfidf_map\n",
    "\n",
    "    def ave_weighted_tfidf(self, tokenized_list):\n",
    "        self.tokenized_list = tokenized_list\n",
    "        weighted_ave = []\n",
    "        tfidf_obj, tfidf_features = self.tfidf_extractor()\n",
    "        word_tfidf_map = self.tfidf_mapper(tfidf_obj, tfidf_features ) \n",
    "\n",
    "        for sentence in tqdm(self.tokenized_list):\n",
    "            word_vecs =  np.array([self.g_model[word] * self.word_in_word_tfidf_map(word, word_tfidf_map)  for word in sentence if word in self.g_model])\n",
    "\n",
    "            weighted_ave.append(np.sum(word_vecs, axis = 0)/len(word_vecs))\n",
    "\n",
    "        return np.array(weighted_ave)\n",
    "    \n",
    "    def word_in_word_tfidf_map(self, word, word_tfidf_map):\n",
    "        if word in word_tfidf_map.values():\n",
    "            return word_tfidf_map[word]\n",
    "        else: \n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1961/1961 [00:48<00:00, 40.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# %load model_ppl.py\n",
    "from helper import *\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from helper import normalize_corpus\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV,cross_val_score,StratifiedShuffleSplit,train_test_split\n",
    "from sklearn.metrics import recall_score,accuracy_score,make_scorer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim import models,corpora\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "class ParsedDataTransformer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"\n",
    "    transform form parsed xml file into topn df\n",
    "    df.content is a string sentence which is not cleaned\n",
    "    df.tags is a list of SINGLE TAG only have top n tags\n",
    "    \"\"\"\n",
    "    def __init__(self, topn_tags):\n",
    "        self.topn_tags = topn_tags\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        X.columns = ['title', 'body', 'tags']\n",
    "        merged = [title + ' ' + body for title, body in zip(X.title, X.body)]\n",
    "        df_merged = pd.DataFrame({'content': merged, 'tags': X.tags.copy()})\n",
    "        data_tags = df_merged.tags.apply(lambda x: x.replace('<','').split('>')[:-1])\n",
    "        new_targets = []\n",
    "        for sample_tags in data_tags:\n",
    "            sample_tags_wanted = []\n",
    "            for tag in sample_tags:\n",
    "                if tag in self.topn_tags:\n",
    "                    sample_tags_wanted.append(tag)\n",
    "            new_targets.append(sample_tags_wanted)\n",
    "        bool_index = [True if t != [] else False for t in new_targets]\n",
    "        rslt = pd.DataFrame({'content': df_merged.content, 'tags': new_targets}).loc[bool_index, :]\n",
    "        return rslt.reset_index(drop=True)\n",
    "\n",
    "\n",
    "class DropRowWithMultipleTags(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"\n",
    "    to reduce overlap, we drop those question with multiple top N tags\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        mask = []\n",
    "        for row in X.tags:\n",
    "            if len(row) == 1:\n",
    "                mask.append(True)\n",
    "            else:\n",
    "                mask.append(False)\n",
    "        return X[mask]\n",
    "\n",
    "\n",
    "\n",
    "class ContentCleaner(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"\n",
    "    using helper function to clean the content text\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        # fully cleaned text\n",
    "        noramalized_docs = normalize_corpus(X.content,tokenize=False)\n",
    "        return pd.DataFrame({'content':noramalized_docs,'tags':X.tags})\n",
    "\n",
    "\n",
    "class BOWVector(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"\n",
    "    to bag of words vector\n",
    "    \"\"\"\n",
    "    def __init__(self,topn_tags):\n",
    "        self.topn_tags = topn_tags\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        content = ' '.join(list(X.content))\n",
    "        uniq_words = set(content.split())\n",
    "        word_to_int = { word:i for i,word in enumerate(uniq_words)}\n",
    "        int_to_word = {i:word for i,word in enumerate(uniq_words)}\n",
    "        content_vecs = []\n",
    "        for sentence in X.content:\n",
    "            vecs = np.zeros(len(uniq_words))\n",
    "            for word in sentence.split():\n",
    "                vecs[word_to_int[word]] += 1\n",
    "            for tag in topn_tags:\n",
    "                vecs[word_to_int[word]] *= 77  # sorry, it's magic number\n",
    "            content_vecs.append(vecs)\n",
    "        return np.array(content_vecs)\n",
    "\n",
    "\n",
    "\n",
    "class ExtractTfidfAveVec(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    get the word to vector model, tfidf model, use them to get the sentence vector\n",
    "    \"\"\"\n",
    "    def __init__(self,vec_len,topn_tags):\n",
    "        self.vec_len = vec_len\n",
    "        self.topn_tags = topn_tags\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.corpus = list(X.content.values.flatten())\n",
    "        self.g_model = gensim.models.Word2Vec([s.split() for s in self.corpus], size=self.vec_len, window=10, min_count=2, sample=1e-3)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.ave_weighted_tfidf(X.content.values.flatten())\n",
    "\n",
    "    def tfidf_extractor(self, ngram_range=(1, 1)):\n",
    "\n",
    "        tfidf_obj = TfidfVectorizer(min_df=3, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "                                    ngram_range=ngram_range, use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                                    stop_words='english')\n",
    "\n",
    "        tfidf_features = tfidf_obj.fit_transform(self.corpus)\n",
    "\n",
    "        return tfidf_obj, tfidf_features\n",
    "\n",
    "    def tfidf_mapper(self, tfidf_obj, tfidf_features):\n",
    "        vocab = tfidf_obj.vocabulary_\n",
    "        words = vocab.keys()\n",
    "        word_tfidfs = [tfidf_features[0, vocab.get(word)] if vocab.get(word) else 0 for word in words]\n",
    "        word_tfidf_map = {word: tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n",
    "\n",
    "        #add more main language weight\n",
    "        for tag in self.topn_tags:\n",
    "            word_tfidf_map[tag] = 100*word_tfidf_map.get(tag,np.zeros(self.vec_len))\n",
    "        return word_tfidf_map\n",
    "\n",
    "    def ave_weighted_tfidf(self, tokenized_list):\n",
    "        self.tokenized_list = tokenized_list\n",
    "        weighted_ave = []\n",
    "        tfidf_obj, tfidf_features = self.tfidf_extractor()\n",
    "        word_tfidf_map = self.tfidf_mapper(tfidf_obj, tfidf_features)\n",
    "\n",
    "        for sentence in tqdm(self.tokenized_list):\n",
    "            word_vecs = np.array(\n",
    "                [self.g_model[word] * self.word_in_word_tfidf_map(word, word_tfidf_map) for word in sentence.split() if\n",
    "                 word in self.g_model])\n",
    "\n",
    "            weighted_ave.append(np.sum(word_vecs, axis=0) / len(word_vecs))\n",
    "\n",
    "        return np.array(weighted_ave)\n",
    "\n",
    "    def word_in_word_tfidf_map(self, word, word_tfidf_map):\n",
    "        if word in word_tfidf_map.values():\n",
    "            return word_tfidf_map[word]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "class LDATransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_topics, passes):\n",
    "        self.num_topics = num_topics\n",
    "        self.passes = passes\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        texts = X.content.apply(lambda x: x.split())\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        print 'training LDA...'\n",
    "        ldamodel = models.ldamodel.LdaModel(\n",
    "            corpus, num_topics=self.num_topics, id2word=dictionary, passes=self.passes)\n",
    "        dict_values = {i: [] for i in range(10)}\n",
    "        for sample in ldamodel.get_document_topics(corpus):\n",
    "            row = np.zeros(self.num_topics)\n",
    "            for topic_id, value in sample:\n",
    "                row[topic_id] = value\n",
    "            for i, v in enumerate(row):\n",
    "                dict_values[i].append(v)\n",
    "        return pd.concat((pd.DataFrame(dict_values), X.iloc[:, 1]), axis=1)\n",
    "\n",
    "\n",
    "def xgb_random_search(x_train,y_train,num_iter=5):\n",
    "    param_distribs = {\n",
    "        'estimator__max_depth': [3, 4],\n",
    "        'estimator__learning_rate': [0.01, 0.05, 0.1, 0.5],\n",
    "        'estimator__n_estimators': list(range(100,1000,100)),\n",
    "        'estimator__colsample_bytree': [0.5,0.7,0.9],\n",
    "    }\n",
    "\n",
    "    gbm = OneVsRestClassifier(xgb.XGBClassifier(),n_jobs=-1)\n",
    "    def score_func(y, y_pred, **kwargs):\n",
    "        return recall_score(y,y_pred,average='macro')\n",
    "    rnd_search = RandomizedSearchCV(gbm, param_distribs, n_iter=num_iter, cv=5,scoring=make_scorer(score_func))\n",
    "    rnd_search.fit(x_train, y_train)\n",
    "\n",
    "    cvres = rnd_search.cv_results_\n",
    "    scores_list = []\n",
    "    for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "        scores_list.append((mean_score, params))\n",
    "    highest_score, highest_paras = sorted(scores_list, key=lambda x: x[0], reverse=True)[0]\n",
    "    print('best score: {}, best paras:{}'.format(highest_score, highest_paras))\n",
    "    return highest_score,highest_paras\n",
    "\n",
    "\n",
    "def svm_random_search(x_train, y_train, num_iter=10):\n",
    "    param_distribs = {\n",
    "        'estimator__kernel':[ 'linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'estimator__C' : list(np.linspace(0.01,1,20)),\n",
    "        \"estimator__degree\": [1, 2, 3, 4],\n",
    "        \"estimator__class_weight\":['balanced'],\n",
    "        \"estimator__probability\":[True]\n",
    "    }\n",
    "\n",
    "    ovsr = OneVsRestClassifier(SVC(),n_jobs=-1)\n",
    "\n",
    "    def score_func(y, y_pred, **kwargs):\n",
    "        return recall_score(y,y_pred,average='macro')\n",
    "\n",
    "    rnd_search = RandomizedSearchCV(ovsr, param_distribs, n_iter=num_iter, cv=5,scoring=make_scorer(score_func))\n",
    "    rnd_search.fit(x_train, y_train)\n",
    "\n",
    "    cvres = rnd_search.cv_results_\n",
    "    scores_list = []\n",
    "    for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "        scores_list.append((mean_score, params))\n",
    "    highest_score, highest_paras = sorted(scores_list, key=lambda x: x[0], reverse=True)[0]\n",
    "    print('best score: {}, best paras:{}'.format(highest_score, highest_paras))\n",
    "    return highest_score,highest_paras\n",
    "\n",
    "\n",
    "class TargetBinerizer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"\n",
    "    transform the target into one-hot like vector\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        mlb = MultiLabelBinarizer().fit(X.tags)\n",
    "        y = mlb.transform(X.tags)\n",
    "        return y,mlb\n",
    "\n",
    "def bool_to_int(array):\n",
    "    rslt = []\n",
    "    for b in array:\n",
    "        rslt.append(int(b))\n",
    "    return rslt\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    df = pd.read_csv('./data/processed/stack_ds_4_9_2017 .csv', quotechar='|', sep=',', header=None )\n",
    "    topn_tags = ['javascript', 'java', 'android', 'php', 'python', 'c#', 'html', 'jquery', 'ios', 'css']\n",
    "\n",
    "\n",
    "    ppl_X = Pipeline([\n",
    "        ('transformer',ParsedDataTransformer(topn_tags)),\n",
    "        ('droprowwithgtonetag',DropRowWithMultipleTags()),\n",
    "        ('textCleaner',ContentCleaner()),\n",
    "        # ('doc2vecTFIDFtransformer',ExtractTfidfAveVec(vec_len=15,topn_tags=topn_tags)),\n",
    "    ])\n",
    "\n",
    "    ppl_y = Pipeline([\n",
    "        ('transformer',ParsedDataTransformer(topn_tags)),\n",
    "        ('droprowwithgtonetag', DropRowWithMultipleTags()),\n",
    "        ('targetBinerizer',TargetBinerizer())\n",
    "    ])\n",
    "\n",
    "\n",
    "    X = ppl_X.fit_transform(df)\n",
    "    y,mlb = ppl_y.fit_transform(df)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer().fit(df.tags)\n",
    "y = mlb.transform(df.tags)\n",
    "mask_is = [bool(x) for x in y[:,-1]]\n",
    "mask_not = [ not bool(x) for x in y[:,-1]]\n",
    "ct_not = CountVectorizer(min_df=3).fit(df.content[mask_not])\n",
    "x_not = ct_not.transform(df.content[mask_not])\n",
    "features_not = ct_not.get_feature_names()\n",
    "\n",
    "ct_is = CountVectorizer(min_df=3).fit(df.content[mask_is])\n",
    "x_is = ct_is.transform(df.content[mask_is])\n",
    "features_is = ct_is.get_feature_names()\n",
    "\n",
    "mapper_is = {w:v for w,v in zip(features_is,np.array(x_is.todense()).sum(axis=0))}\n",
    "mapper_not = {w:v for w,v in zip(features_not,np.array(x_not.todense()).sum(axis=0))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['android', 'c#', 'css', 'html', 'ios', 'java', 'javascript',\n",
       "       'jquery', 'php', 'python'], dtype=object)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratios = []\n",
    "for union_word in set(features_is) & set(features_not):\n",
    "    ratio = float(mapper_is[union_word])/float(mapper_not[union_word]) * x_not.shape[0] / x_is.shape[0]\n",
    "    ratio = np.log(ratio)\n",
    "    if ratio >= 1:\n",
    "        ratios.append((ratio,union_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratios = sorted(ratios,key=lambda x:x[0],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6.095574280864156, u'python'),\n",
       " (3.2215389422876846, u'csv'),\n",
       " (2.8315780207154853, u'typeerror'),\n",
       " (2.6645239360523192, u'shell'),\n",
       " (2.2407096892759584, u'matrix'),\n",
       " (2.212538812309262, u'relative'),\n",
       " (2.1637486481398298, u'pointer'),\n",
       " (2.0583881324820035, u'layer'),\n",
       " (1.9406050968256201, u'ps'),\n",
       " (1.9042374526547454, u'corresponding')]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}